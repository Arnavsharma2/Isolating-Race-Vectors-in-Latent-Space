{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disentangled Representation Learning\n",
    "\n",
    "Steps performed:\n",
    "1. Load Stable Diffusion model\n",
    "2. Generate or encode images\n",
    "3. Extract race vector\n",
    "4. Generate counterfactuals\n",
    "5. Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ── Colab / Local Setup ──────────────────────────────────────────────────────\n# Run this cell first. It clones the repo and installs dependencies on Colab,\n# and is a no-op when running locally.\n\nimport os, sys\n\nIN_COLAB = \"google.colab\" in str(get_ipython()) if hasattr(__builtins__, \"__import__\") else False\ntry:\n    from IPython import get_ipython\n    IN_COLAB = \"google.colab\" in str(get_ipython())\nexcept Exception:\n    IN_COLAB = False\n\nif IN_COLAB:\n    REPO = \"Isolating-Race-Vectors-in-Latent-Space\"\n    if not os.path.exists(REPO):\n        os.system(f\"git clone https://github.com/Arnavsharma2/Isolating-Race-Vectors-in-Latent-Space.git\")\n    os.chdir(REPO)\n    os.system(\"pip install -q -r requirements.txt\")\n    print(\"Colab setup complete. GPU available:\", os.popen(\"nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null\").read().strip() or \"None detected\")\nelse:\n    # Running locally — make sure we're in the repo root\n    repo_root = os.path.dirname(os.path.abspath(\"__file__\"))\n    if \"notebooks\" in repo_root:\n        os.chdir(os.path.dirname(repo_root))\n    sys.path.insert(0, \"..\")\n    print(\"Running locally.\")\n\nprint(\"Working directory:\", os.getcwd())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom src.models.stable_diffusion import StableDiffusionWrapper\nfrom src.latent.vector_discovery import RaceVectorExtractor, VectorAnalyzer\nfrom src.metrics.evaluator import CounterfactualEvaluator\nfrom src.visualization.grid_generator import CounterfactualGridGenerator\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.stable_diffusion import StableDiffusionWrapper\n",
    "from src.latent.vector_discovery import RaceVectorExtractor\n",
    "from src.latent.manipulator import LatentManipulator\n",
    "from src.metrics.evaluator import CounterfactualEvaluator\n",
    "from src.visualization.grid_generator import CounterfactualGridGenerator\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading Stable Diffusion XL from stabilityai/stable-diffusion-xl-base-1.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c7a039607944d8bd8e220c5c9a61a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Stable Diffusion loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = StableDiffusionWrapper(\n",
    "    device=device,\n",
    "    dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    enable_xformers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Test Images\n",
    "\n",
    "We'll generate a few images with different racial attributes to extract the race vector."
   ]
  },
  {
   "cell_type": "code",
   "source": "# ── Photo Upload (Colab only) ─────────────────────────────────────────────────\n# Skip this cell when running locally — just put your photos directly in\n#   data/photos/light_skin/  and  data/photos/dark_skin/\n#\n# On Colab: run this cell to upload photos via the file picker.\n# Need at least 3 photos per group for a reliable race vector.\n\nif IN_COLAB:\n    from google.colab import files\n    import os\n\n    os.makedirs(\"data/photos/light_skin\", exist_ok=True)\n    os.makedirs(\"data/photos/dark_skin\", exist_ok=True)\n\n    print(\"── Step 1: Upload LIGHT SKIN photos ──\")\n    uploaded_light = files.upload()\n    for name, data in uploaded_light.items():\n        with open(f\"data/photos/light_skin/{name}\", \"wb\") as f:\n            f.write(data)\n    print(f\"Saved {len(uploaded_light)} light-skin photo(s).\\n\")\n\n    print(\"── Step 2: Upload DARK SKIN photos ──\")\n    uploaded_dark = files.upload()\n    for name, data in uploaded_dark.items():\n        with open(f\"data/photos/dark_skin/{name}\", \"wb\") as f:\n            f.write(data)\n    print(f\"Saved {len(uploaded_dark)} dark-skin photo(s).\")\nelse:\n    print(\"Local run — photos should already be in data/photos/light_skin/ and dark_skin/\")\n    print(\"Light skin:\", len(list(__import__('pathlib').Path('data/photos/light_skin').glob('*.*'))), \"files\")\n    print(\"Dark skin: \", len(list(__import__('pathlib').Path('data/photos/dark_skin').glob('*.*'))), \"files\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 light skin photos\n",
      "Encoding image copy 2.png...\n",
      "Encoding image copy.png...\n",
      "Encoding image.png...\n",
      "Found 3 dark skin photos\n",
      "Encoding image copy 2.png...\n",
      "Encoding image copy.png...\n",
      "Encoding image.png...\n",
      "\n",
      "✓ Loaded 3 light and 3 dark photos\n"
     ]
    }
   ],
   "source": [
    "# USING REAL PHOTOS - This cell loads your downloaded photos\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Setup paths to your photo directories\n",
    "LIGHT_PHOTOS_DIR = Path(\"../data/photos/light_skin\")\n",
    "DARK_PHOTOS_DIR = Path(\"../data/photos/dark_skin\")\n",
    "\n",
    "# Load light skin photos (supports JPG, JPEG, PNG)\n",
    "light_images = []\n",
    "light_latents = []\n",
    "light_files = (list(LIGHT_PHOTOS_DIR.glob(\"*.jpg\")) + \n",
    "               list(LIGHT_PHOTOS_DIR.glob(\"*.jpeg\")) + \n",
    "               list(LIGHT_PHOTOS_DIR.glob(\"*.png\")))\n",
    "\n",
    "print(f\"Found {len(light_files)} light skin photos\")\n",
    "\n",
    "for img_path in sorted(light_files)[:10]:  # Limit to 10\n",
    "    print(f\"Encoding {img_path.name}...\")\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    # Resize to 512x512 for SD\n",
    "    img = img.resize((512, 512), Image.LANCZOS)\n",
    "    latent = model.encode_image(img)\n",
    "    light_images.append(img)\n",
    "    light_latents.append(latent)\n",
    "\n",
    "# Load dark skin photos  \n",
    "dark_images = []\n",
    "dark_latents = []\n",
    "dark_files = (list(DARK_PHOTOS_DIR.glob(\"*.jpg\")) + \n",
    "              list(DARK_PHOTOS_DIR.glob(\"*.jpeg\")) + \n",
    "              list(DARK_PHOTOS_DIR.glob(\"*.png\")))\n",
    "\n",
    "print(f\"Found {len(dark_files)} dark skin photos\")\n",
    "\n",
    "for img_path in sorted(dark_files)[:10]:  # Limit to 10\n",
    "    print(f\"Encoding {img_path.name}...\")\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = img.resize((512, 512), Image.LANCZOS)\n",
    "    latent = model.encode_image(img)\n",
    "    dark_images.append(img)\n",
    "    dark_latents.append(latent)\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(light_images)} light and {len(dark_images)} dark photos\")\n",
    "\n",
    "if len(light_images) == 0 or len(dark_images) == 0:\n",
    "    print(\"\\n⚠️  WARNING: No photos loaded!\")\n",
    "    print(f\"   Light skin directory: {LIGHT_PHOTOS_DIR.absolute()}\")\n",
    "    print(f\"   Dark skin directory: {DARK_PHOTOS_DIR.absolute()}\")\n",
    "    print(\"   Make sure you have photos in both folders!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating light skin image 1/3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d042dc10314452d8517e2ab4673c732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(prompts_light):\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating light skin image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompts_light)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     img, lat = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_from_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdark skin, tan skin, brown skin, tanned, multiple people, accessories, jewelry, glasses, shadows\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Higher guidance for stronger adherence to prompt\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     light_images.append(img)\n\u001b[32m     22\u001b[39m     light_latents.append(lat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Isolating-Race-Vectors-in-Latent-Space/notebooks/../src/models/stable_diffusion.py:241\u001b[39m, in \u001b[36mStableDiffusionWrapper.generate_from_prompt\u001b[39m\u001b[34m(self, prompt, negative_prompt, num_inference_steps, guidance_scale, seed, return_latent, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m     generator = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Generate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpil\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreturn_latent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlatent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_latent:\n\u001b[32m    252\u001b[39m     \u001b[38;5;66;03m# Safely extract latent from output\u001b[39;00m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(output, \u001b[33m'\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:1217\u001b[39m, in \u001b[36mStableDiffusionXLPipeline.__call__\u001b[39m\u001b[34m(self, prompt, prompt_2, height, width, num_inference_steps, timesteps, sigmas, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[39m\n\u001b[32m   1215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ip_adapter_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ip_adapter_image_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1216\u001b[39m     added_cond_kwargs[\u001b[33m\"\u001b[39m\u001b[33mimage_embeds\u001b[39m\u001b[33m\"\u001b[39m] = image_embeds\n\u001b[32m-> \u001b[39m\u001b[32m1217\u001b[39m noise_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1227\u001b[39m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_classifier_free_guidance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_condition.py:1215\u001b[39m, in \u001b[36mUNet2DConditionModel.forward\u001b[39m\u001b[34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[39m\n\u001b[32m   1212\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) > \u001b[32m0\u001b[39m:\n\u001b[32m   1213\u001b[39m         additional_residuals[\u001b[33m\"\u001b[39m\u001b[33madditional_residuals\u001b[39m\u001b[33m\"\u001b[39m] = down_intrablock_additional_residuals.pop(\u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1215\u001b[39m     sample, res_samples = \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_blocks.py:1270\u001b[39m, in \u001b[36mCrossAttnDownBlock2D.forward\u001b[39m\u001b[34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[39m\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1269\u001b[39m     hidden_states = resnet(hidden_states, temb)\n\u001b[32m-> \u001b[39m\u001b[32m1270\u001b[39m     hidden_states = \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1279\u001b[39m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[38;5;28mlen\u001b[39m(blocks) - \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m additional_residuals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/diffusers/models/transformers/transformer_2d.py:427\u001b[39m, in \u001b[36mTransformer2DModel.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[39m\n\u001b[32m    416\u001b[39m         hidden_states = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    417\u001b[39m             block,\n\u001b[32m    418\u001b[39m             hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    424\u001b[39m             class_labels,\n\u001b[32m    425\u001b[39m         )\n\u001b[32m    426\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m         hidden_states = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# 3. Output\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_input_continuous:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/diffusers/models/attention.py:995\u001b[39m, in \u001b[36mBasicTransformerBlock.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels, added_cond_kwargs)\u001b[39m\n\u001b[32m    992\u001b[39m cross_attention_kwargs = cross_attention_kwargs.copy() \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    993\u001b[39m gligen_kwargs = cross_attention_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mgligen\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn1\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43monly_cross_attention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm_type == \u001b[33m\"\u001b[39m\u001b[33mada_norm_zero\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1003\u001b[39m     attn_output = gate_msa.unsqueeze(\u001b[32m1\u001b[39m) * attn_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/diffusers/models/attention_processor.py:605\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001b[39m\n\u001b[32m    600\u001b[39m     logger.warning(\n\u001b[32m    601\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcross_attention_kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are not expected by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.processor.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and will be ignored.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    602\u001b[39m     )\n\u001b[32m    603\u001b[39m cross_attention_kwargs = {k: w \u001b[38;5;28;01mfor\u001b[39;00m k, w \u001b[38;5;129;01min\u001b[39;00m cross_attention_kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m attn_parameters}\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/diffusers/models/attention_processor.py:2765\u001b[39m, in \u001b[36mAttnProcessor2_0.__call__\u001b[39m\u001b[34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, *args, **kwargs)\u001b[39m\n\u001b[32m   2761\u001b[39m     key = attn.norm_k(key)\n\u001b[32m   2763\u001b[39m \u001b[38;5;66;03m# the output of sdp = (batch, num_heads, seq_len, head_dim)\u001b[39;00m\n\u001b[32m   2764\u001b[39m \u001b[38;5;66;03m# TODO: add support for attn.scale when we move to Torch 2.1\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2765\u001b[39m hidden_states = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   2767\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2769\u001b[39m hidden_states = hidden_states.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).reshape(batch_size, -\u001b[32m1\u001b[39m, attn.heads * head_dim)\n\u001b[32m   2770\u001b[39m hidden_states = hidden_states.to(query.dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Generate images with light skin tone\n",
    "# Using VERY SPECIFIC prompts with extreme contrasts for better vector extraction\n",
    "light_images = []\n",
    "light_latents = []\n",
    "\n",
    "prompts_light = [\n",
    "    \"portrait photo of a caucasian person with extremely pale white fair skin, light complexion, neutral expression, studio lighting, plain white background, professional photography\",\n",
    "    \"headshot photograph of a person with very light fair skin tone and pale complexion, neutral face, even lighting, simple backdrop, high detail\",\n",
    "    \"professional portrait of a light-skinned person with fair pale complexion, centered, neutral background, sharp focus\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts_light):\n",
    "    print(f\"Generating light skin image {i+1}/{len(prompts_light)}...\")\n",
    "    img, lat = model.generate_from_prompt(\n",
    "        prompt, \n",
    "        negative_prompt=\"dark skin, tan skin, brown skin, tanned, multiple people, accessories, jewelry, glasses, shadows\",\n",
    "        seed=42+i, \n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=8.0,  # Higher guidance for stronger adherence to prompt\n",
    "    )\n",
    "    light_images.append(img)\n",
    "    light_latents.append(lat)\n",
    "\n",
    "# Generate images with dark skin tone\n",
    "dark_images = []\n",
    "dark_latents = []\n",
    "\n",
    "prompts_dark = [\n",
    "    \"portrait photo of an african person with extremely dark deep black skin, very dark complexion, neutral expression, studio lighting, plain white background, professional photography\",\n",
    "    \"headshot photograph of a person with very dark deep skin tone and black complexion, neutral face, even lighting, simple backdrop, high detail\",\n",
    "    \"professional portrait of a dark-skinned person with deep black complexion, centered, neutral background, sharp focus\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts_dark):\n",
    "    print(f\"Generating dark skin image {i+1}/{len(prompts_dark)}...\")\n",
    "    img, lat = model.generate_from_prompt(\n",
    "        prompt, \n",
    "        negative_prompt=\"light skin, pale skin, fair skin, white skin, caucasian, multiple people, accessories, jewelry, glasses, shadows\",\n",
    "        seed=1042+i, \n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=8.0,  # Higher guidance\n",
    "    )\n",
    "    dark_images.append(img)\n",
    "    dark_latents.append(lat)\n",
    "\n",
    "print(\"Done!\")\n",
    "print()\n",
    "print(\"IMPORTANT: Visually inspect the images below!\")\n",
    "print(\"- Light images should have VERY pale/white skin\")\n",
    "print(\"- Dark images should have VERY dark/black skin\")\n",
    "print(\"- If they look similar, the race vector will not work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generated images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for i in range(3):\n",
    "    axes[0, i].imshow(light_images[i])\n",
    "    axes[0, i].set_title(f\"Light Skin {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i in range(3):\n",
    "    axes[1, i].imshow(dark_images[i])\n",
    "    axes[1, i].set_title(f\"Dark Skin {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"These images will be used to extract the race vector.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Check if latents are actually different\n",
    "print(\"=\" * 60)\n",
    "print(\"DIAGNOSTIC: Checking latent differences\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute average latent for each group\n",
    "avg_light = torch.stack(light_latents).mean(dim=0)\n",
    "avg_dark = torch.stack(dark_latents).mean(dim=0)\n",
    "\n",
    "# Compute raw difference\n",
    "raw_diff = avg_dark - avg_light\n",
    "\n",
    "print(f\"Average light latent: mean={avg_light.mean().item():.4f}, std={avg_light.std().item():.4f}\")\n",
    "print(f\"Average dark latent:  mean={avg_dark.mean().item():.4f}, std={avg_dark.std().item():.4f}\")\n",
    "print(f\"Raw difference:       mean={raw_diff.mean().item():.4f}, std={raw_diff.std().item():.4f}\")\n",
    "print(f\"Difference magnitude: {raw_diff.norm().item():.4f}\")\n",
    "print()\n",
    "\n",
    "# Check if the difference is meaningful\n",
    "if raw_diff.norm().item() < 10.0:\n",
    "    print(\"⚠️  WARNING: Latent differences are very small!\")\n",
    "    print(\"   This means the generated images are too similar.\")\n",
    "    print(\"   The race vector will NOT work properly.\")\n",
    "    print()\n",
    "    print(\"   SOLUTIONS:\")\n",
    "    print(\"   1. Check the generated images - do they look different?\")\n",
    "    print(\"   2. Try more extreme prompts (see suggestions below)\")\n",
    "    print(\"   3. Use real photos instead of generated images\")\n",
    "else:\n",
    "    print(\"✓ Latent differences look reasonable\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Compute average skin color from images\n",
    "import numpy as np\n",
    "\n",
    "def get_center_region_color(img):\n",
    "    \"\"\"Extract average color from center region (face area)\"\"\"\n",
    "    img_array = np.array(img)\n",
    "    h, w = img_array.shape[:2]\n",
    "    # Center 50% region\n",
    "    y1, y2 = h//4, 3*h//4\n",
    "    x1, x2 = w//4, 3*w//4\n",
    "    center = img_array[y1:y2, x1:x2]\n",
    "    return center.mean(axis=(0, 1))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUAL DIAGNOSTIC: Average skin colors\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get average colors\n",
    "light_colors = [get_center_region_color(img) for img in light_images]\n",
    "dark_colors = [get_center_region_color(img) for img in dark_images]\n",
    "\n",
    "avg_light_color = np.mean(light_colors, axis=0)\n",
    "avg_dark_color = np.mean(dark_colors, axis=0)\n",
    "\n",
    "print(f\"Light skin avg RGB: {avg_light_color}\")\n",
    "print(f\"Dark skin avg RGB:  {avg_dark_color}\")\n",
    "print(f\"Difference:         {avg_light_color - avg_dark_color}\")\n",
    "print()\n",
    "\n",
    "# Check brightness difference (simple metric)\n",
    "light_brightness = avg_light_color.mean()\n",
    "dark_brightness = avg_dark_color.mean()\n",
    "brightness_diff = light_brightness - dark_brightness\n",
    "\n",
    "print(f\"Light brightness: {light_brightness:.1f}\")\n",
    "print(f\"Dark brightness:  {dark_brightness:.1f}\")\n",
    "print(f\"Difference:       {brightness_diff:.1f}\")\n",
    "print()\n",
    "\n",
    "if abs(brightness_diff) < 20:\n",
    "    print(\"⚠️  CRITICAL: Images are TOO SIMILAR in brightness!\")\n",
    "    print(\"   Difference < 20 means race vector will be weak/broken\")\n",
    "    print()\n",
    "    print(\"   ACTION REQUIRED:\")\n",
    "    print(\"   1. Look at the images above - do they actually look different?\")\n",
    "    print(\"   2. If not different enough, regenerate with more extreme prompts\")\n",
    "    print(\"   3. OR use real photographs instead of generated images\")\n",
    "elif abs(brightness_diff) < 40:\n",
    "    print(\"⚠️  Warning: Images have small brightness difference\")\n",
    "    print(\"   Race vector may be weak. Ideal difference: 50-100+\")\n",
    "else:\n",
    "    print(f\"✓ Good brightness contrast! ({abs(brightness_diff):.1f})\")\n",
    "    print(\"  Race vector should work well\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Race Vector\n",
    "\n",
    "Compute the average difference between light and dark skin latent codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Improvements to Prevent \"Black Halo/Fog\" Artifacts\n",
    "\n",
    "To ensure proper disentanglement and avoid background artifacts:\n",
    "\n",
    "1. **Balanced Spatial Masking**: \n",
    "   - `edge_weight=0.0` ensures NO changes to background\n",
    "   - `radius=0.8` creates a smooth, natural transition (larger = smoother)\n",
    "   - Gaussian falloff prevents visible circle boundaries\n",
    "   - **Key tradeoff**: Larger radius = smoother transition but may affect some background\n",
    "   \n",
    "2. **Proportionally Smaller Alphas**: \n",
    "   - Since the mask concentrates the vector, we need smaller alpha values\n",
    "   - Using `±0.8` instead of `±2.0` for natural results\n",
    "   \n",
    "3. **Better Prompts**: Using specific prompts focused ONLY on skin tone differences\n",
    "\n",
    "4. **Negative Prompts**: Explicitly excluding unwanted variations\n",
    "\n",
    "5. **More Training Examples**: Using 3 examples per group for robust extraction\n",
    "\n",
    "**Understanding the Radius Parameter**:\n",
    "- **Small (0.3-0.5)**: Very tight focus, may create visible circle, needs tiny alphas\n",
    "- **Medium (0.6-0.8)**: Balanced - smooth transition, good for most cases ✓\n",
    "- **Large (0.9-1.2)**: Very smooth but may affect background edges\n",
    "\n",
    "**If you see artifacts**:\n",
    "- **Black circle/ring**: Radius too small OR alphas too large\n",
    "  → Try `radius=0.8` with `alphas = [-0.8, -0.4, 0.0, 0.4, 0.8]`\n",
    "- **Black fog**: Radius too large OR edge_weight > 0\n",
    "  → Try `radius=0.6` with `edge_weight=0.0`\n",
    "- **No visible change**: Alphas too small\n",
    "  → Increase gradually: try `[-1.0, -0.5, 0.0, 0.5, 1.0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEBUGGING START (Agent Injected) ---\n",
    "import torch\n",
    "if 'light_latents' in locals() and len(light_latents) > 0:\n",
    "    l_stack = torch.stack(light_latents).float()\n",
    "    d_stack = torch.stack(dark_latents).float()\n",
    "    print(f'Light Latents: Mean={l_stack.mean():.4f}, Std={l_stack.std():.4f}, Range=[{l_stack.min():.4f}, {l_stack.max():.4f}]')\n",
    "    print(f'Dark Latents:  Mean={d_stack.mean():.4f}, Std={d_stack.std():.4f}, Range=[{d_stack.min():.4f}, {d_stack.max():.4f}]')\n",
    "    diff = d_stack - l_stack\n",
    "    print(f'Raw Difference: Mean={diff.mean():.4f}, Norm={diff.norm():.4f}')\n",
    "else:\n",
    "    print('WARNING: Latents not found or empty.')\n",
    "# --- DEBUGGING END ---\n",
    "\n",
    "# --- DEBUGGING START (Agent Injected) ---\n",
    "import torch\n",
    "if 'light_latents' in locals() and len(light_latents) > 0:\n",
    "    l_stack = torch.stack(light_latents).float()\n",
    "    d_stack = torch.stack(dark_latents).float()\n",
    "    print(f'Light Latents: Mean={l_stack.mean():.4f}, Std={l_stack.std():.4f}, Range=[{l_stack.min():.4f}, {l_stack.max():.4f}]')\n",
    "    print(f'Dark Latents:  Mean={d_stack.mean():.4f}, Std={d_stack.std():.4f}, Range=[{d_stack.min():.4f}, {d_stack.max():.4f}]')\n",
    "    diff = d_stack - l_stack\n",
    "    print(f'Raw Difference: Mean={diff.mean():.4f}, Norm={diff.norm():.4f}')\n",
    "else:\n",
    "    print('WARNING: Latents not found or empty.')\n",
    "# --- DEBUGGING END ---\n",
    "\n",
    "extractor = RaceVectorExtractor(device=device)\n",
    "\n",
    "# Create spatial mask to focus on face region (center) and minimize background effects\n",
    "# Using a LARGER radius with gaussian falloff for natural-looking transitions\n",
    "latent_shape = light_latents[0].shape  # (4, H, W) or (1, 4, H, W)\n",
    "if len(latent_shape) == 4:\n",
    "    h, w = latent_shape[-2], latent_shape[-1]\n",
    "else:\n",
    "    h, w = latent_shape[-2], latent_shape[-1]\n",
    "\n",
    "spatial_mask = extractor.create_center_mask(\n",
    "    height=h,\n",
    "    width=w,\n",
    "    center_weight=1.0,\n",
    "    edge_weight=0.0,      # Zero weight at edges - NO background changes\n",
    "    falloff='gaussian',   # Smooth falloff (NOT hard - avoids visible circles)\n",
    "    radius=0.8,          # LARGER radius for smoother, more natural transition\n",
    ")\n",
    "\n",
    "print(f\"Created spatial mask with shape: {spatial_mask.shape}\")\n",
    "print(f\"Mask: center={spatial_mask[h//2, w//2].item():.4f}, edge={spatial_mask[0, 0].item():.4f}\")\n",
    "\n",
    "race_vector = extractor.extract_from_pairs(\n",
    "    light_latents,\n",
    "    dark_latents,\n",
    "    normalize=False,  # Preserve magnitude for better control\n",
    "    spatial_mask=spatial_mask,\n",
    ")\n",
    "\n",
    "print(f\"Race vector shape: {race_vector.shape}\")\n",
    "print(f\"Race vector norm: {race_vector.norm().item():.4f}\")\n",
    "print(f\"✓ Using larger radius (0.8) with smooth Gaussian falloff\")\n",
    "\n",
    "# --- ANALYSIS & FIX START (Agent Injected) ---\n",
    "print(f'Race Vector: Norm={race_vector.norm():.4f}, Mean={race_vector.mean():.4f}')\n",
    "if race_vector.norm() < 1.0 or race_vector.norm() > 1000.0:\n",
    "    print('⚠️  Vector magnitude suspect. Force-enabling normalization for stability.')\n",
    "    race_vector = extractor.extract_from_pairs(\n",
    "        light_latents, dark_latents, normalize=True, spatial_mask=spatial_mask\n",
    "    )\n",
    "    print(f'New Normalized Vector Norm: {race_vector.norm():.4f}')\n",
    "# --- ANALYSIS & FIX END ---\n",
    "\n",
    "# --- ANALYSIS & FIX START (Agent Injected) ---\n",
    "print(f'Race Vector: Norm={race_vector.norm():.4f}, Mean={race_vector.mean():.4f}')\n",
    "if race_vector.norm() < 1.0 or race_vector.norm() > 1000.0:\n",
    "    print('⚠️  Vector magnitude suspect. Force-enabling normalization for stability.')\n",
    "    race_vector = extractor.extract_from_pairs(\n",
    "        light_latents, dark_latents, normalize=True, spatial_mask=spatial_mask\n",
    "    )\n",
    "    print(f'New Normalized Vector Norm: {race_vector.norm():.4f}')\n",
    "# --- ANALYSIS & FIX END ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the spatial mask\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Spatial mask\n",
    "axes[0].imshow(spatial_mask.cpu().numpy(), cmap='hot')\n",
    "axes[0].set_title('Spatial Mask\\n(High weight = center/face, Low weight = edges/background)')\n",
    "axes[0].set_xlabel('Width')\n",
    "axes[0].set_ylabel('Height')\n",
    "plt.colorbar(axes[0].images[0], ax=axes[0], label='Weight')\n",
    "\n",
    "# Analyze vector properties\n",
    "from src.latent.vector_discovery import VectorAnalyzer\n",
    "\n",
    "analyzer = VectorAnalyzer(device=device)\n",
    "analysis = analyzer.analyze_spatial_pattern(race_vector)\n",
    "\n",
    "# Visualize spatial heatmap (averaged across channels)\n",
    "axes[1].imshow(analysis['spatial_heatmap'].cpu().numpy(), cmap='hot')\n",
    "axes[1].set_title('Race Vector Spatial Activation Pattern\\n(After masking)')\n",
    "axes[1].set_xlabel('Width')\n",
    "axes[1].set_ylabel('Height')\n",
    "plt.colorbar(axes[1].images[0], ax=axes[1], label='Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total magnitude: {analysis['total_magnitude']:.4f}\")\n",
    "print(f\"Center-focused: The mask ensures changes concentrate on the face region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Generate counterfactuals using steered denoising\n# Each image is generated from the same seed as the base image, with the race\n# vector injected at every denoising step. This avoids the \"misty\" VAE\n# artifacts that come from directly adding a vector to a final latent.\n\nalphas = [-4, -2, 0, 2, 4]\nBASE_PROMPT = \"portrait photo of a person, professional headshot, neutral background, high quality, detailed face\"\nBASE_SEED = 999\n\nprint(\"Generating counterfactuals (steered denoising)...\")\nprint(\"Alpha values:\", alphas)\nprint(\"Negative = lighter skin, Positive = darker skin\\n\")\n\ncounterfactual_images = []\n\nfor alpha in alphas:\n    print(f\"  α = {alpha:+.1f} ...\", end=\" \", flush=True)\n\n    if abs(alpha) < 0.01:\n        counterfactual_images.append(base_image)\n        print(\"(base image, skipped)\")\n        continue\n\n    img, _ = model.generate_steered(\n        prompt=BASE_PROMPT,\n        race_vector=race_vector,\n        alpha=alpha,\n        seed=BASE_SEED,\n        negative_prompt=\"multiple people, accessories, jewelry, glasses\",\n        num_inference_steps=50,\n        guidance_scale=7.5,\n    )\n    counterfactual_images.append(img)\n    print(\"done\")\n\nprint(f\"\\nGenerated {len(counterfactual_images)} counterfactual images.\")\nprint(\"Tip — if the effect is too subtle: try alphas = [-6, -3, 0, 3, 6]\")\nprint(\"Tip — if the face changes too much: try alphas = [-2, -1, 0, 1, 2]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate base image with similar quality settings\n",
    "print(\"Generating base image...\")\n",
    "base_image, base_latent = model.generate_from_prompt(\n",
    "    \"portrait photo of a person, professional headshot, neutral background, high quality, detailed face\",\n",
    "    negative_prompt=\"multiple people, accessories, jewelry, glasses\",\n",
    "    seed=999,\n",
    "    num_inference_steps=50,  # Same as training images\n",
    "    guidance_scale=7.5,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(base_image)\n",
    "plt.title(\"Base Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Tuning Cell ───────────────────────────────────────────────────────────────\n# Adjust alpha range and mask settings here if results aren't right.\n# Run this cell to regenerate with new settings, then re-run the viz cell.\n\nALPHA_RANGE  = 4    # symmetric: generates [-ALPHA_RANGE, ..., 0, ..., +ALPHA_RANGE]\nMASK_RADIUS  = 1.0  # 0.6–1.2 recommended; larger = smoother spatial transition\nEDGE_WEIGHT  = 0.3  # 0.0 = pure center mask, 0.3 = soft blend at edges\n\nprint(\"=\" * 60)\nprint(\"TUNING PARAMETERS\")\nprint(f\"  Alpha range : ±{ALPHA_RANGE}\")\nprint(f\"  Mask radius : {MASK_RADIUS}\")\nprint(f\"  Edge weight : {EDGE_WEIGHT}\")\nprint(\"=\" * 60)\n\n# Re-extract race vector with updated mask\nlatent_shape = light_latents[0].shape\nh, w = latent_shape[-2], latent_shape[-1]\n\nspatial_mask = extractor.create_center_mask(\n    height=h, width=w,\n    center_weight=1.0,\n    edge_weight=EDGE_WEIGHT,\n    falloff=\"gaussian\",\n    radius=MASK_RADIUS,\n)\n\nrace_vector = extractor.extract_from_pairs(\n    light_latents, dark_latents,\n    normalize=False,\n    spatial_mask=spatial_mask,\n)\nprint(f\"Re-extracted vector (norm: {race_vector.norm().item():.4f})\")\n\n# Re-generate counterfactuals with updated alpha range\nalphas = [-ALPHA_RANGE, -ALPHA_RANGE // 2, 0, ALPHA_RANGE // 2, ALPHA_RANGE]\ncounterfactual_images = []\n\nfor alpha in alphas:\n    print(f\"  α = {alpha:+.1f} ...\", end=\" \", flush=True)\n    if abs(alpha) < 0.01:\n        counterfactual_images.append(base_image)\n        print(\"(base)\")\n        continue\n    img, _ = model.generate_steered(\n        prompt=BASE_PROMPT,\n        race_vector=race_vector,\n        alpha=alpha,\n        seed=BASE_SEED,\n        negative_prompt=\"multiple people, accessories, jewelry, glasses\",\n        num_inference_steps=50,\n        guidance_scale=7.5,\n    )\n    counterfactual_images.append(img)\n    print(\"done\")\n\nprint(\"\\nDone. Run the visualization cell below to see results.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quick Fix: If You See Black Fog/Halos\n",
    "\n",
    "# If counterfactuals show artifacts, uncomment and run this cell to re-extract with stronger masking:\n",
    "\n",
    "\n",
    "# Re-extract with hard mask (sharp cutoff)\n",
    "latent_shape = light_latents[0].shape\n",
    "h, w = (latent_shape[-2], latent_shape[-1]) if len(latent_shape) == 4 else (latent_shape[-2], latent_shape[-1])\n",
    "\n",
    "spatial_mask_hard = extractor.create_center_mask(\n",
    "    height=h, width=w,\n",
    "    center_weight=1.0, edge_weight=0.0,\n",
    "    falloff='hard',  # Sharp cutoff instead of gaussian\n",
    "    radius=0.4,      # Even tighter focus (vs 0.5)\n",
    ")\n",
    "race_vector = extractor.extract_from_pairs(\n",
    "    light_latents, dark_latents,\n",
    "    normalize=False, spatial_mask=spatial_mask_hard,\n",
    ")\n",
    "print(f\"Re-extracted with hard mask: {race_vector.norm().item():.4f}\")\n",
    "\n",
    "# Then re-run the counterfactual generation with smaller alphas\n",
    "alphas = [-1.0, -0.5, 0.0, 0.5, 1.0]\n",
    "# ... (continue with counterfactual generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNING CELL: Adjust these parameters if you see artifacts\n",
    "# This cell is ready to run - just modify the values below\n",
    "\n",
    "# Current settings (modify as needed):\n",
    "USE_HARD_MASK = False  # Set to True for sharp cutoff (may create visible circle)\n",
    "MASK_RADIUS = 0.8      # 0.6-0.8 recommended, larger = smoother\n",
    "ALPHA_SCALE = 0.8      # Scale factor for alphas (0.5 = half strength, 1.5 = 1.5x strength)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TUNING PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if USE_HARD_MASK or MASK_RADIUS != 0.8 or ALPHA_SCALE != 0.8:\n",
    "    print(f\"Re-extracting with custom settings...\")\n",
    "    print(f\"  Mask type: {'HARD' if USE_HARD_MASK else 'Gaussian'}\")\n",
    "    print(f\"  Radius: {MASK_RADIUS}\")\n",
    "    print(f\"  Alpha scale: {ALPHA_SCALE}\")\n",
    "    print()\n",
    "    \n",
    "    latent_shape = light_latents[0].shape\n",
    "    h, w = (latent_shape[-2], latent_shape[-1]) if len(latent_shape) == 4 else (latent_shape[-2], latent_shape[-1])\n",
    "    \n",
    "    spatial_mask = extractor.create_center_mask(\n",
    "        height=h, width=w,\n",
    "        center_weight=1.0, \n",
    "        edge_weight=0.0,\n",
    "        falloff='hard' if USE_HARD_MASK else 'gaussian',\n",
    "        radius=MASK_RADIUS,\n",
    "    )\n",
    "    \n",
    "    # Re-extract vector\n",
    "    race_vector = extractor.extract_from_pairs(\n",
    "        light_latents, dark_latents,\n",
    "        normalize=False, \n",
    "        spatial_mask=spatial_mask,\n",
    "    )\n",
    "    print(f\"✓ Re-extracted vector (norm: {race_vector.norm().item():.4f})\")\n",
    "    \n",
    "    # Use scaled alphas\n",
    "    base_alphas = [-0.8, -0.4, 0.0, 0.4, 0.8]\n",
    "    alphas = [a * ALPHA_SCALE for a in base_alphas]\n",
    "    print(f\"✓ Using alphas: {[f'{a:.2f}' for a in alphas]}\")\n",
    "    \n",
    "    # Re-generate counterfactuals\n",
    "    counterfactual_latents = manipulator.generate_counterfactuals(base_latent, race_vector, alphas)\n",
    "    counterfactual_images = [model.decode_latent(lat) for lat in counterfactual_latents]\n",
    "    \n",
    "    print(f\"✓ Re-generated counterfactuals\")\n",
    "    print()\n",
    "    print(\"Next: Run the visualization cell below to see results\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"Using default settings (no changes)\")\n",
    "    print(\"To tune, modify the values at the top of this cell\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize counterfactuals\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for i, (img, alpha) in enumerate(zip(counterfactual_images, alphas)):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"α = {alpha:.1f}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Results\n",
    "\n",
    "Measure identity preservation and disentanglement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = CounterfactualEvaluator(device=device)\n",
    "\n",
    "# Evaluate each counterfactual (skip α=0)\n",
    "print(\"Evaluating counterfactuals...\\n\")\n",
    "\n",
    "results = []\n",
    "for i, (cf_image, alpha) in enumerate(zip(counterfactual_images, alphas)):\n",
    "    if abs(alpha) < 0.01:  # Skip original\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nEvaluating α = {alpha:.1f}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result = evaluator.evaluate_pair(\n",
    "        base_image,\n",
    "        cf_image,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([r.to_dict() for r in results])\n",
    "df['alpha'] = [a for a in alphas if abs(a) >= 0.01]\n",
    "\n",
    "# Plot metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Face similarity\n",
    "axes[0, 0].plot(df['alpha'], df['face_similarity'], 'o-')\n",
    "axes[0, 0].axhline(y=0.85, color='r', linestyle='--', label='Threshold')\n",
    "axes[0, 0].set_xlabel('Alpha')\n",
    "axes[0, 0].set_ylabel('Face Similarity')\n",
    "axes[0, 0].set_title('Identity Preservation')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Landmark RMSE\n",
    "if df['landmark_rmse'].notna().any():\n",
    "    axes[0, 1].plot(df['alpha'], df['landmark_rmse'], 'o-')\n",
    "    axes[0, 1].axhline(y=5.0, color='r', linestyle='--', label='Threshold')\n",
    "    axes[0, 1].set_xlabel('Alpha')\n",
    "    axes[0, 1].set_ylabel('Landmark RMSE (px)')\n",
    "    axes[0, 1].set_title('Facial Geometry Preservation')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "# Background SSIM\n",
    "if df['background_ssim'].notna().any():\n",
    "    axes[1, 0].plot(df['alpha'], df['background_ssim'], 'o-')\n",
    "    axes[1, 0].axhline(y=0.90, color='r', linestyle='--', label='Threshold')\n",
    "    axes[1, 0].set_xlabel('Alpha')\n",
    "    axes[1, 0].set_ylabel('Background SSIM')\n",
    "    axes[1, 0].set_title('Background Preservation')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "# Overall score\n",
    "axes[1, 1].plot(df['alpha'], df['overall_score'], 'o-')\n",
    "axes[1, 1].set_xlabel('Alpha')\n",
    "axes[1, 1].set_ylabel('Overall Score')\n",
    "axes[1, 1].set_title('Overall Disentanglement Quality')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Visualization Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = CounterfactualGridGenerator()\n",
    "\n",
    "# Create grid (excluding α=0)\n",
    "cf_images_no_orig = [img for img, a in zip(counterfactual_images, alphas) if abs(a) >= 0.01]\n",
    "labels = [f\"α={a:.1f}\" for a in alphas if abs(a) >= 0.01]\n",
    "metrics_list = [r.to_dict() for r in results]\n",
    "\n",
    "grid = generator.generate_grid(\n",
    "    base_image,\n",
    "    cf_images_no_orig,\n",
    "    labels=labels,\n",
    "    metrics=metrics_list,\n",
    "    title=\"Disentangled Race Vector Demonstration\",\n",
    ")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.imshow(grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Save\n",
    "grid.save('../experiments/results/demo_grid.png')\n",
    "print(\"Grid saved to: experiments/results/demo_grid.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}