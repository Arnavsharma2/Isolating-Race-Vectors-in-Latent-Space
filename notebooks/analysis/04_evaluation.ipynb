{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Comparatice Evaluation: Race Vector vs Prompt Engineering\n",
                "\n",
                "This notebook runs the rigorous evaluation comparing our Latent Race Vector method against a standard Prompt Engineering baseline.\n",
                "\n",
                "We measure:\n",
                "- **Identity Preservation:** Face similarity, LPIPS\n",
                "- **Structural Consistency:** SSIM, Pose difference\n",
                "- **Disentanglement:** Metric scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from IPython.display import display, Image\n",
                "\n",
                "# Add src to path\n",
                "sys.path.insert(0, '../../')\n",
                "\n",
                "from src.evaluation.comparative_eval import ComparativeEvaluator\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Run Evaluation Pipeline\n",
                "This will take some time as it generates images and computes metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "evaluator = ComparativeEvaluator()\n",
                "\n",
                "# Run comparison (adjust num_samples as needed for speed vs rigor)\n",
                "df_results = evaluator.run_comparison(num_samples=10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Quantitative Results\n",
                "Comparison of average metrics between methods."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load results if not running fresh\n",
                "results_path = '../../experiments/comparative_results/comparative_metrics.csv'\n",
                "if os.path.exists(results_path):\n",
                "    df_results = pd.read_csv(results_path)\n",
                "\n",
                "# Group by method and target race\n",
                "summary = df_results.groupby(['method', 'target_race'])[\n",
                "    ['face_similarity', 'background_ssim', 'overall_score', 'overall_ssim', 'lpips']\n",
                "].mean()\n",
                "\n",
                "display(summary)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Visual Qualitative Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "\n",
                "output_dir = '../../experiments/comparative_results'\n",
                "vector_samples = sorted(glob.glob(f\"{output_dir}/vector_sample_*.png\"))[:4]\n",
                "prompt_samples = sorted(glob.glob(f\"{output_dir}/prompt_sample_*.png\"))[:4]\n",
                "\n",
                "print(\"Vector Modification Samples:\")\n",
                "for p in vector_samples:\n",
                "    display(Image(filename=p))\n",
                "\n",
                "print(\"\\nPrompt Engineering Samples:\")\n",
                "for p in prompt_samples:\n",
                "    display(Image(filename=p))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}